{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Gradient descent</h1>\n",
    "\n",
    "<style>\n",
    "    h1{\n",
    "        color: lightseagreen;\n",
    "        width: 100%;\n",
    "        text-align: center;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used for training, and used in deep learning models.\n",
    "\n",
    "<img src=\"images/39.png\">\n",
    "\n",
    "J might not always be a bow shape.\n",
    "\n",
    "example of a more complex surface plot J:\n",
    "\n",
    "<img src=\"images/40.png\" style=\"float: left; margin-right: 10px\">\n",
    "This is not squared error cost function, for linear regression with squared error cost function you always end up with a bow shape or a hammer shape, This is a type of cost function you might get if you are a neural network model.\n",
    "\n",
    "The axis are `w` and `b`, the height of the surface is the cost of the output function.\n",
    "\n",
    "imagine standing on the top of the hill (like the image), and you start getting to the bottom as efficiently as possible.\n",
    "\n",
    "so what the cradient descent do you're going to spin arround and see what direction to go, if i want to go downhill by doing baby-steps down hill.\n",
    "\n",
    "after taking the first step we repeat the process to continue going downhill, until we reach the local minimum.\n",
    "\n",
    "This is multiple steps of ***Gradient descent***.\n",
    "\n",
    "now imagine if we try gradient descent again, but this time we choose a different starting point, a couple steps to the right then you end up in a different valley, a different minimum (in blue).\n",
    "\n",
    "the bottoms of the valleys are called the local minima.\n",
    "\n",
    "\n",
    "<style>\n",
    "    img {\n",
    "        width: 400px;\n",
    "        height: 300px;\n",
    "        border: 1px solid lightseagreen;\n",
    "        border-radius: 10px;\n",
    "    }\n",
    "    h2 {\n",
    "        margin-left: 20px;\n",
    "        color: lightseagreen;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implementing gradient descent</h2>\n",
    "\n",
    "<img src=\"images/41.png\" style=\"float: left; margin-right: 10px\">\n",
    "\n",
    "on each step w is updated to the old value of w minus alpha * derivative of J.\n",
    "\n",
    "Means update W by taking it's current value, and adjusting it a small ammount.\n",
    "\n",
    "first the `=` notation is the assignment operator, `α` is the learning rate(small number *0 < α < 1*), it take control how big of a step we take downhill (how agressive the gradient descent is), the last term, is the derivative of $J_{(w,b)}$.\n",
    "\n",
    "the `b` is assigned to a new value to.\n",
    "\n",
    "here we are updating both values.\n",
    "\n",
    "the preupdated w is what goes into the derivative term of J over b\n",
    "\n",
    "if we update value first then the derrivative of J over b, this is incorrect.\n",
    "\n",
    "<h2>Quiz<h2>\n",
    "\n",
    "<img src=\"images/42.png\">\n",
    "\n",
    "\n",
    "<style>\n",
    "    img {\n",
    "        width: 400px;\n",
    "        height: 300px;\n",
    "        border: 1px solid lightseagreen;\n",
    "        border-radius: 10px;\n",
    "    }\n",
    "    h2 {\n",
    "        margin-left: 20px;\n",
    "        color: lightseagreen;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gradient descent intuition</h2>\n",
    "\n",
    "<img src=\"images/42.png\">\n",
    "\n",
    "we can look a 2d graph instead of 3d graph\n",
    "\n",
    "<img src=\"images/43.png\">\n",
    "\n",
    "a way to think of the derivative at this point of the line is to draw a tangente line (in math the slope of the line is the derivative of the function at this point, height/width), if the slope is to the right then the derivative is positive and vise-versa, so the value of `w` will decrease and the cost will decrease as when going to the left.\n",
    "\n",
    "same for the second example.\n",
    "\n",
    "<h2>Quiz</h2>\n",
    "\n",
    "<img src=\"images/44.png\">\n",
    "\n",
    "\n",
    "<style>\n",
    "    img {\n",
    "        width: 400px;\n",
    "        height: 300px;\n",
    "        border: 1px solid lightseagreen;\n",
    "        border-radius: 10px;\n",
    "    }\n",
    "    h2 {\n",
    "        margin-left: 20px;\n",
    "        color: lightseagreen;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning rate</h2>\n",
    "\n",
    "<img src=\"images/45.png\">\n",
    "\n",
    "If the learning rate is to small, you take tiny steps so it take alot of steps and alot of time.\n",
    "\n",
    "If the learning rate is to large, you update w by giant steps it can miss the minimum and make the cost worse, and it get worse and worse.\n",
    "\n",
    "Another question if the value is at a local minimum.\n",
    "\n",
    "<img src=\"images/46.png\">\n",
    "\n",
    "the value of w is at 5 on the right which is local minimum, the slope is 0 (the partial derivative is 0), so w stop changing with gradient descent.\n",
    "\n",
    "But the gradient descent can reach local min even with fixed learning rate :\n",
    "\n",
    "<img src=\"images/47.png\">\n",
    "\n",
    "As we get closer to the minimum the derivative get smaller, closer to 0.\n",
    "\n",
    "<img src=\"images/48.png\">\n",
    "\n",
    "<style>\n",
    "    img {\n",
    "        width: 400px;\n",
    "        height: 300px;\n",
    "        border: 1px solid lightseagreen;\n",
    "        border-radius: 10px;\n",
    "    }\n",
    "    h2 {\n",
    "        margin-left: 20px;\n",
    "        color: lightseagreen;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gradient descent for linear regression</h2>\n",
    "\n",
    "<img src=\"images/49.png\">\n",
    "\n",
    "here some math:\n",
    "\n",
    "<img src=\"images/50.png\">\n",
    "\n",
    "partial derivative with respect to b *d*/$d_{w}$ b is the constant.\n",
    "\n",
    "repeat gradient descent algorithm untile convergence.\n",
    "\n",
    "<img src=\"images/51.png\">\n",
    "\n",
    "There is more than one local minimum, depending where you initialize w and b, you end up at different local minimum:\n",
    "\n",
    "<img src=\"images/52.png\">\n",
    "\n",
    "When you're using a squared cost function with linear regression the cost function will nevere have multiple local minimum.\n",
    "\n",
    "<img src=\"images/53.png\">\n",
    "\n",
    "\n",
    "\n",
    "<h2>Running gradient descent</h2>\n",
    "\n",
    "<img src=\"images/54.png\">\n",
    "\n",
    "here we have contour plot of the cost function and surface plot of the same function.\n",
    "\n",
    "First we have w = -0.1 and b = 900\n",
    "\n",
    "then we have more steps\n",
    "\n",
    "<img src=\"images/55.png\">\n",
    "\n",
    "the line fit the data better and better until global minimum.\n",
    "\n",
    "this process called ***Batch*** gradient descent, where each step look at the entire training example.\n",
    "\n",
    "<img src=\"images/56.png\">\n",
    "\n",
    "\n",
    "\n",
    "<style>\n",
    "    img {\n",
    "        width: 500px;\n",
    "        height: 340px;\n",
    "        border: 1px solid lightseagreen;\n",
    "        border-radius: 10px;\n",
    "    }\n",
    "    h2 {\n",
    "        margin-left: 20px;\n",
    "        color: lightseagreen;\n",
    "    }\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
