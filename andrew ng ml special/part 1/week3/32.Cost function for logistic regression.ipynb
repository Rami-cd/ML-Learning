{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cost function for logistic regression</h1>\n",
    "\n",
    "<style>\n",
    "    h1{\n",
    "        color: lightseagreen;\n",
    "        width: 100%;\n",
    "        text-align: center;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/93.png\">\n",
    "\n",
    "here a training set for a classification probleme.\n",
    "\n",
    "<img src=\"images/94.png\">\n",
    "\n",
    "i linear regression the cost function is convex, but in classifiction it's not convex.\n",
    "\n",
    "if we use gradient descent there is a lot of local minima and you can get stuck in, so this squarred error cost function is not a good choice.\n",
    "\n",
    "on new terms, the inside is called the loss.\n",
    "\n",
    "<img src=\"images/95.png\">\n",
    "\n",
    "now the cost function take as input the f(x) and the true label y and tells us how well we are doing in this example.\n",
    "\n",
    "why this function make sense, when we zoom in the loss function get closer to 0 as the value approach 1.\n",
    "\n",
    "<img src=\"images/96.png\">\n",
    "\n",
    "now if y = 0 the cost get lower when the value goes to 0.\n",
    "\n",
    "so this is why the squarred loss function won't work.\n",
    "\n",
    "using this new function it's convex so we can reach a global minimum.\n",
    "\n",
    "<img src=\"images/97.png\">\n",
    "\n",
    "<style>\n",
    "    img {\n",
    "        width: 500px;\n",
    "        height: 340px;\n",
    "        border: 1px solid lightseagreen;\n",
    "        border-radius: 10px;\n",
    "    }\n",
    "    h2 {\n",
    "        margin-left: 20px;\n",
    "        color: lightseagreen;\n",
    "    }\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
