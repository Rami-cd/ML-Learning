{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: lightseagreen\">Backpropagation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<h3 style=\"color: lightseagreen\">Reminder of the cost function</h3> The cost function we will search which weights and biases minimize the cost function.\n",
    "\n",
    "**The cost of a single training example**: The output that the network give - the output you want it to give, then add the squares of the differences between each component.\n",
    "doing this for all the training examples and averaging the results will give you the total cost of the network.\n",
    "\n",
    "**Solution**: we want the negative gradient of this cost function which tells you how you need to change these weights and biases, to get the most afficient decrease of the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: lightseagreen\">Backpropagation</h3>\n",
    "\n",
    "Backpropagation is an algorithm for computing that crazy complicated gradient.\n",
    "\n",
    "**Recap**\n",
    "the magnitude of each component here is telling you how sensitive the cost function to each weight and bias.\n",
    "\n",
    "<img src=\"images/Screenshot (219).png\">\n",
    "\n",
    "**For example**\n",
    "Here the cost of the function is 32 times more sensitive to the changes in that first weight, 32 greater than the changes of the second weights.\n",
    "\n",
    "<img src=\"images/Screenshot (220).png\">\n",
    "\n",
    "<h3 style=\"color: lightseagreen\">Intuitive walthrough</h3>\n",
    "\n",
    "Focus on the example:\n",
    "\n",
    "<img src=\"images/Screenshot (221).png\">\n",
    "\n",
    "Let's say that we are at a point where the network is not well trained yet, so the activations on the ouput gonna look pretty ramdom.\n",
    "\n",
    "<img src=\"images/Screenshot (222).png\">\n",
    "<img src=\"images/Screenshot (223).png\">\n",
    "\n",
    "But it's helpful to keep track of which adjustments we which should take place to that output layer.\n",
    "\n",
    "<img src=\"images/Screenshot (224).png\">\n",
    "\n",
    "we want the third value to nudge up while the others to nudge down.\n",
    "\n",
    "<img src=\"images/Screenshot (225).png\">\n",
    "\n",
    "Moreover the sizes of these nudges should be propotional to how far away each current value is from it's target value.\n",
    "\n",
    "**For example**:\n",
    "the increase to that number 2 neuron activation is more important than the decrease of number 2 neuron.\n",
    "\n",
    "<img src=\"images/Screenshot (226).png\">\n",
    "<img src=\"images/Screenshot (227).png\">\n",
    "\n",
    "Focusing on the weights change\n",
    "\n",
    "<img src=\"images/Screenshot (228).png\">\n",
    "\n",
    "the connections with the brightest neurons, has the biggest effect since those weights are multipliyed by larger activation values, so increasing those weights has a strong influence on the ultimate cost function, than increasing the weights of the weights of connections with dimmer neurons.\n",
    "\n",
    "**When** we talk about gradient descent we don't just care about wetheir each component should be nudget up of down, we care about which one gives you the most bang for your buck.\n",
    "\n",
    "**Theory**\n",
    "<div style=\"color: white;\n",
    "            border: 1px solid;\">\n",
    "this is somehow similar to a theory in biological science\n",
    "\n",
    "<img src=\"images/Screenshot (229).png\">\n",
    "\n",
    "Here the biggest increases to weights, the biggest strenthening of connections, happens between neurons which are the most active and the ones we wish to become more active.\n",
    "\n",
    "In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about the 2.\n",
    "</div>\n",
    "\n",
    "**The third way is by changing the activations in the previous layer**\n",
    "\n",
    "Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active.\n",
    "\n",
    "<img src=\"images/Screenshot (230).png\">\n",
    "\n",
    "And similar to the weight changes, you're going to get the most bang for your buck, by seeking changes that are propotional to the size of the corresponding weights.\n",
    "\n",
    "we cannot control those activations we have control over the weights and biases.\n",
    "\n",
    "**it's helpful to keep a note of what those desired changes are**\n",
    "\n",
    "we still have the rest of the output neurons\n",
    "\n",
    "<img src=\"images/Screenshot (232).png\">\n",
    "\n",
    "and each of those ouput neurons has its own thoughts about what should happen to that second to last layer.\n",
    "\n",
    "So, the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in propotion to the corresponding weights, and in propotion to how much each of those neurons needs to change.\n",
    "\n",
    "<img src=\"images/Screenshot (233).png\">\n",
    "\n",
    "this is the idea of propagating backwards comes in.\n",
    "\n",
    "By adding together all these desired effects, you get a list of nudges of that you want to happen to the second to last layer, and once you have those you can recursivly apply the same process to the relevant weights and biases that determin those values, repeating the same process, and moving backward through the network.\n",
    "\n",
    "<img src=\"images/Screenshot (234).png\">\n",
    "<img src=\"images/Screenshot (235).png\">\n",
    "<img src=\"images/Screenshot (236).png\">\n",
    "\n",
    "This is how a single training example wishes to nudge each one of those weights and biases, if we only listen to the two the network will always classify as a 2.\n",
    "\n",
    "So what you do is you go through the same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and you average together those desired changes.\n",
    "\n",
    "<img src=\"images/Screenshot (237).png\">\n",
    "\n",
    "This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced before, or at least something propotional to it.\n",
    "\n",
    "<h2 style=\"color: lightseagreen\">Summary</h2>\n",
    "\n",
    "Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, ...\n",
    "\n",
    "<style>\n",
    "    img {\n",
    "        width: 400px;\n",
    "        height: 310px;\n",
    "        border: 1px solid lightseagreen;\n",
    "    }\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
