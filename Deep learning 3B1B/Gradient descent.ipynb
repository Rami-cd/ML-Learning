{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: lightseagreen\">Gradient Descent</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will know how the network learn.\n",
    "\n",
    "we will show the network a whole bunch of training data <br> come in the form of images of hand written digits along with labels with what are they supposed to be.\n",
    "\n",
    "it will adjust it weight and biases to improve in the training data hopefully<br>\n",
    "this layered structure will generalize to images beyond that training data.\n",
    "\n",
    "<img src=\"images/Screenshot (188).png\">\n",
    "\n",
    "<img src=\"images/Screenshot (189).png\">\n",
    "\n",
    "<img src=\"images/Screenshot (190).png\">\n",
    "\n",
    "<img src=\"images/Screenshot (191).png\">\n",
    "\n",
    "we will start with random variables, the output will be a mess, so we define a cost function.<br> it's a way of telling the computer that he's wrong.\n",
    "\n",
    "<img src=\"images/Screenshot (193).png\">\n",
    "\n",
    "mathematically you add up the squares of the differences between each of these trash output<br>activations and the values that you want them to have, this<br>what we call the cost of a single training example.\n",
    "\n",
    "the sum is low when the network classify the  image correctly :\n",
    "\n",
    "<img src=\"images/Screenshot (194).png\">\n",
    "\n",
    "but it's large when the network seem tha he doesn't know :\n",
    "\n",
    "<img src=\"images/Screenshot (195).png\">\n",
    "\n",
    "now you consider the average cost, this is a mesure for how lousy the network is, (and how bad the computer should feel).\n",
    "\n",
    "<img src=\"images/Screenshot (196).png\">\n",
    "\n",
    "<img src=\"images/Screenshot (197).png\">\n",
    "\n",
    "instead of looking at the cost function as a function with a lot of inputs, look at it as a<br> simple function and you want to find the input that minimalize the value of this function\n",
    "\n",
    "<img src=\"images/Screenshot (198).png\">\n",
    "\n",
    "you can sometimes figure this min explicitly.\n",
    "\n",
    "<img src=\"images/Screenshot (199).png\">\n",
    "\n",
    "but that not alwayd feasable for complicated functions.\n",
    "\n",
    "a simple solution is to start with an input, \n",
    "and figure out <br> which direction you shoud step.\n",
    "\n",
    "<img src=\"images/Screenshot (200).png\">\n",
    "\n",
    "if you could fiure out the slope you can see wich direction to go to.\n",
    "**negative to the right positive to the left**\n",
    "\n",
    "<img src=\"images/Screenshot (201).png\">\n",
    "<img src=\"images/Screenshot (202).png\">\n",
    "<img src=\"images/Screenshot (203).png\">\n",
    "\n",
    "imagine a ball rolling down a hill.\n",
    "\n",
    "but there is a lot of input and there is no guarrantie that it will land on the lowest point.\n",
    "\n",
    "* Local minimum = Doable\n",
    "* Global minimum = Crazy hard\n",
    "\n",
    "if you make yout step sized prpotional to the slope, when the slope is flattening out toward<br>the minimum your steps get smaller and smaller.\n",
    "\n",
    "<img src=\"images/Screenshot (204).png\">\n",
    "\n",
    "we can think of it as a function with two inputs and one output\n",
    "\n",
    "<img src=\"images/Screenshot (205).png\">\n",
    "\n",
    "taking the opposite of the gradient gives negative, it give you wich direction of steepest descend (vice versa).\n",
    "\n",
    "<img src=\"images/Screenshot (206).png\">\n",
    "\n",
    "<img src=\"images/Screenshot (207).png\">\n",
    "\n",
    "<img src=\"images/Screenshot (208).png\">\n",
    "\n",
    "**the heart of how a neural network learn is the Backpropagation.**\n",
    "\n",
    "what we mean by a network learning is we are minimizing cost function.\n",
    "\n",
    "<img src=\"images/Screenshot (209).png\">\n",
    "\n",
    "we can think about like this, earch component of the negative gradient tells us 2 things,<br>\n",
    "the sign tells us whether the corresponding component of the input vector should be nodget up of down.<br>\n",
    "the magnitude of these components tells you wich change matter more.\n",
    "\n",
    "<img src=\"images/Screenshot (210).png\">\n",
    "\n",
    "in network the adjustment of a weight might have a greater inpact on the cost function than some other wieghts.\n",
    "\n",
    "<img src=\"images/Screenshot (211).png\">\n",
    "\n",
    "another way to think about this cost function is it encodes the relative importance of each weight and bias\n",
    "\n",
    "<img src=\"images/Screenshot (212).png\">\n",
    "<img src=\"images/Screenshot (213).png\">\n",
    "\n",
    "<h2 style=\"color: lightseagreen\">Summary</h2>\n",
    "\n",
    "<img src=\"images/Screenshot (214).png\">\n",
    "<img src=\"images/Screenshot (215).png\">\n",
    "<img src=\"images/Screenshot (216).png\">\n",
    "\n",
    "But this model, has fund a local minimum helped him to classify most images.<br>\n",
    "but it doesn't pick up on the pattern that we haped for (like rings, certain shapes),<br>\n",
    "if we input a random image it give you a nonsense answer, even if it can recognize digits, it has no idea how to draw them,<br>\n",
    "because it's tightly constraint training setup.\n",
    "\n",
    "This is not the endgoal this network is just a starting point, this is a old technologie<br> needed to understand, to understand more detailed modern variance.\n",
    "\n",
    "<img src=\"images/Screenshot (217).png\">\n",
    "\n",
    "<style>\n",
    "    img {\n",
    "        width: 400px;\n",
    "        height: 310px;\n",
    "        border: 1px solid lightseagreen;\n",
    "    }\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
